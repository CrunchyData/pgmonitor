groups:
- name: alert-rules
  rules:
      
########## EXPORTER RULES ##########
  - alert: PGExporterScrapeError
    expr: pg_exporter_last_scrape_error > 0
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      summary: 'Postgres Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing queries. Error count: ( {{ $value }} )'

  - alert: NodeExporterScrapeError
    expr: node_textfile_scrape_error > 0
    for: 60s
    labels:
      service: system 
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      summary: 'Node Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing custom metrics.  Error count: ( {{ $value }} )'


########## POSTGRESQL RULES ##########
  - alert: PGIsUp
    expr: pg_up < 1
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      summary: 'postgres_exporter running on {{ $labels.job }} is unable to communicate with the configured database'


# Whether a system switches from primary to replica or vice versa must be configured per named job.
# No way to tell what value a system is supposed to be without a rule expression for that specific system
# 2 to 1 means it changed from primary to replica. 1 to 2 means it changed from replica to primary
# Set this alert for each system that you want to monitor a recovery status change
# Below is an example for a target job called "Prod" and watches for the value to change to 1

#  - alert: PGRecoveryStatusSwitch_Prod
#    expr: ccp_is_in_recovery_status{job="Prod"} < 2
#    for: 60s
#    labels:
#        service: postgresql
#        severity: critical
#        severity_num: 300
#        alert_value: '{{ $value }}'
#    annonations: 
#       summary: '{{ $labels.job }} has changed from primary to replica'


# Absence alerts must be configured per named job, otherwise there's no way to know which job is down
# Below is an example for a target job called "Prod"
#  - alert: PGConnectionAbsent
#    expr: absent(ccp_connection_stats_max_connections{job="Prod"})
#    for: 10s
#    labels:
#      service: postgresql
#      severity: critical
#      severity_num: 300
#      alert_value: '{{ $value }}'
#    annotations:
#      description: 'Connection metric is absent from target (Prod). Check that postgres_exporter can connect to PostgreSQL.'



  - alert: PGIdleTxn
    expr: ccp_connection_stats_max_idle_in_txn_time > 300
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} has at least one session idle in transaction for over 5 minutes.'
      summary: 'PGSQL Instance idle transactions'

  - alert: PGIdleTxn
    expr: ccp_connection_stats_max_idle_in_txn_time > 900
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} has at least one session idle in transaction for over 15 minutes.'
      summary: 'PGSQL Instance idle transactions'

  - alert: PGQueryTime
    expr: ccp_connection_stats_max_query_time > 43200
    for: 60s
    labels:
      service: postgresql
      severity: warning 
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} has at least one query running for over 12 hours.'
      summary: 'PGSQL Max Query Runtime'

  - alert: PGQueryTime
    expr: ccp_connection_stats_max_query_time > 86400 
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} has at least one query running for over 1 day.'
      summary: 'PGSQL Max Query Runtime'

  - alert: PGConnPerc
    expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 75
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} is using 75% or more of available connections ({{ $value }}%)'
      summary: 'PGSQL Instance connections'

  - alert: PGConnPerc
    expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 90 
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: '{{ $labels.job }} is using 90% or more of available connections ({{ $value }}%)'
      summary: 'PGSQL Instance connections'

  - alert: PGDBSize
    expr: ccp_database_size > 1.073741824e+11
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} over 100GB in size: {{ $value }} bytes'
      summary: 'PGSQL Instance size warning'

  - alert: PGDBSize
    expr: ccp_database_size > 2.68435456e+11
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} over 250GB in size: {{ $value }} bytes'
      summary: 'PGSQL Instance size critical'

  - alert: PGReplicationByteLag
    expr: ccp_replication_status_byte_lag > 5.24288e+07
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind.'
      summary: 'PGSQL Instance replica lag warning'

  - alert: PGReplicationByteLag
    expr: ccp_replication_status_byte_lag > 1.048576e+08
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind.'
      summary: 'PGSQL Instance replica lag warning'

  - alert: PGReplicationSlotsInactive
    expr: ccp_replication_slots_active == 0
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has one or more inactive replication slots'
      summary: 'PGSQL Instance inactive replication slot'

  - alert: PGXIDWraparound
    expr: ccp_transaction_wraparound_percent_towards_wraparound > 50
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound.'
      summary: 'PGSQL Instance {{ $labels.job }} transaction id wraparound imminent'

  - alert: PGXIDWraparound
    expr: ccp_transaction_wraparound_percent_towards_wraparound > 75
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound.'
      summary: 'PGSQL Instance transaction id wraparound imminent'

  - alert: PGEmergencyVacuum
    expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 75
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 75% towards emergency autovacuum processes beginning'
      summary: 'PGSQL Instance emergency vacuum imminent'

  - alert: PGEmergencyVacuum
    expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 90
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 90% towards emergency autovacuum processes beginning'
      summary: 'PGSQL Instance emergency vacuum imminent'

########## SYSTEM RULES ##########
  - alert: ExporterDown
    expr: avg_over_time(up[5m]) < 0.9 
    for: 10s 
    labels:
      service: system
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'Metrics exporter service for {{ $labels.job }} running on {{ $labels.instance }} has been down at least 50% of the time for the last 5 minutes. Service may be flapping or down.'
      summary: 'Prometheus Exporter Service Down'

  - alert: DiskUsagePerc
    expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}) BY (job,device)) > 70
    for: 2m
    labels:
      service: system
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'Disk usage on target {{ $labels.job }} at {{ $value }}%'

  - alert: DiskUsagePerc
    expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}) BY (job,device)) > 85
    for: 2m
    labels:
      service: system
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'Disk usage on target {{ $labels.job }} at {{ $value }}%'

  - alert: DiskFillPredict
    expr: predict_linear(node_filesystem_free_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}[1h], 4 * 3600) < 0
    for: 5m
    labels:
      service: system
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: '(EXPERIMENTAL) Disk {{ $labels.device }} on target {{ $labels.job }} is predicted to fill in 4 hrs based on current usage'

  - alert: SystemLoad5m
    expr: node_load5 > 5
    for: 10m
    labels:
      service: system
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'System load for target {{ $labels.job }} is high ({{ $value }})'

  - alert: SystemLoad5m
    expr: node_load5 > 10
    for: 10m
    labels:
      service: system
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'System load for target {{ $labels.job }} is high ({{ $value }})'

  - alert: MemoryAvailable
    expr: (100 * (node_memory_Available_bytes) / node_memory_MemTotal_bytes) < 25
    for: 1m
    labels:
      service: system
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'Memory available for target {{ $labels.job }} is at {{ $value }}%'

  - alert: MemoryAvailable
    expr: (100 * (node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) < 10
    for: 1m
    labels:
      service: system
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'Memory available for target {{ $labels.job }} is at {{ $value }}%'

  - alert: SwapUsage
    expr: (100 - (100 * (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes))) > 60
    for: 1m
    labels:
      service: system
      severity: warning
      severity_num: 200
      alert_value: '{{ $value }}'
    annotations:
      description: 'Swap usage for target {{ $labels.job }} is at {{ $value }}%'

  - alert: SwapUsage
    expr: (100 - (100 * (node_memory_SwapFree_byte / node_memory_SwapTotal_bytes))) > 80
    for: 1m
    labels:
      service: system
      severity: critical
      severity_num: 300
      alert_value: '{{ $value }}'
    annotations:
      description: 'Swap usage for target {{ $labels.job }} is at {{ $value }}%'
