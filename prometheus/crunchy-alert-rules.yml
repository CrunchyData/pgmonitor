groups:
- name: alert-rules
  rules:
      
########## EXPORTER RULES ##########
  - alert: PGExporterScrapeError
    expr: pg_exporter_last_scrape_error > 0
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      summary: 'Postgres Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing queries. Error count: ( {{ $value }} )'

  - alert: NodeExporterScrapeError
    expr: node_textfile_scrape_error > 0
    for: 60s
    labels:
      service: system 
      severity: critical
      severity_num: 300
    annotations:
      summary: 'Node Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing custom metrics.  Error count: ( {{ $value }} )'



########## POSTGRESQL RULES ##########
  - alert: PGIsUp
    expr: pg_up < 1
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      summary: 'postgres_exporter running on {{ $labels.job }} is unable to communicate with the configured database'


# Whether a system switches from primary to replica or vice versa must be configured per named job.
# No way to tell what value a system is supposed to be without a rule expression for that specific system
# 2 to 1 means it changed from primary to replica. 1 to 2 means it changed from replica to primary
# Set this alert for each system that you want to monitor a recovery status change
# Below is an example for a target job called "Replica" and watches for the value to change above 1 which means it's no longer a replica
#
#  - alert: PGRecoveryStatusSwitch_Replica 
#    expr: ccp_is_in_recovery_status{job="Replica"} > 1 
#    for: 60s
#    labels:
#      service: postgresql
#      severity: critical
#      severity_num: 300
#    annotations:
#      summary: '{{ $labels.job }} has changed from replica to primary'


# Absence alerts must be configured per named job, otherwise there's no way to know which job is down
# Below is an example for a target job called "Prod"
#  - alert: PGConnectionAbsent
#    expr: absent(ccp_connection_stats_max_connections{job="Prod"})
#    for: 10s
#    labels:
#      service: postgresql
#      severity: critical
#      severity_num: 300
#    annotations:
#      description: 'Connection metric is absent from target (Prod). Check that postgres_exporter can connect to PostgreSQL.'



  - alert: PGIdleTxn
    expr: ccp_connection_stats_max_idle_in_txn_time > 300
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: '{{ $labels.job }} has at least one session idle in transaction for over 5 minutes.'
      summary: 'PGSQL Instance idle transactions'

  - alert: PGIdleTxn
    expr: ccp_connection_stats_max_idle_in_txn_time > 900
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: '{{ $labels.job }} has at least one session idle in transaction for over 15 minutes.'
      summary: 'PGSQL Instance idle transactions'

  - alert: PGQueryTime
    expr: ccp_connection_stats_max_query_time > 43200
    for: 60s
    labels:
      service: postgresql
      severity: warning 
      severity_num: 200
    annotations:
      description: '{{ $labels.job }} has at least one query running for over 12 hours.'
      summary: 'PGSQL Max Query Runtime'

  - alert: PGQueryTime
    expr: ccp_connection_stats_max_query_time > 86400 
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: '{{ $labels.job }} has at least one query running for over 1 day.'
      summary: 'PGSQL Max Query Runtime'

  - alert: PGConnPerc
    expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 75
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: '{{ $labels.job }} is using 75% or more of available connections ({{ $value }}%)'
      summary: 'PGSQL Instance connections'

  - alert: PGConnPerc
    expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) > 90 
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: '{{ $labels.job }} is using 90% or more of available connections ({{ $value }}%)'
      summary: 'PGSQL Instance connections'

  - alert: PGDBSize
    expr: ccp_database_size > 1.073741824e+11
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} over 100GB in size: {{ $value }} bytes'
      summary: 'PGSQL Instance size warning'

  - alert: PGDBSize
    expr: ccp_database_size > 2.68435456e+11
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} over 250GB in size: {{ $value }} bytes'
      summary: 'PGSQL Instance size critical'

  - alert: PGReplicationByteLag
    expr: ccp_replication_status_byte_lag > 5.24288e+07
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind.'
      summary: 'PGSQL Instance replica lag warning'

  - alert: PGReplicationByteLag
    expr: ccp_replication_status_byte_lag > 1.048576e+08
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind.'
      summary: 'PGSQL Instance replica lag warning'

  - alert: PGReplicationSlotsInactive
    expr: ccp_replication_slots_active == 0
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} has one or more inactive replication slots'
      summary: 'PGSQL Instance inactive replication slot'

  - alert: PGXIDWraparound
    expr: ccp_transaction_wraparound_percent_towards_wraparound > 50
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound.'
      summary: 'PGSQL Instance {{ $labels.job }} transaction id wraparound imminent'

  - alert: PGXIDWraparound
    expr: ccp_transaction_wraparound_percent_towards_wraparound > 75
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound.'
      summary: 'PGSQL Instance transaction id wraparound imminent'

  - alert: PGEmergencyVacuum
    expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 75
    for: 60s
    labels:
      service: postgresql
      severity: warning
      severity_num: 200
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 75% towards emergency autovacuum processes beginning'
      summary: 'PGSQL Instance emergency vacuum imminent'

  - alert: PGEmergencyVacuum
    expr: ccp_transaction_wraparound_percent_towards_emergency_autovac > 90
    for: 60s
    labels:
      service: postgresql
      severity: critical
      severity_num: 300
    annotations:
      description: 'PGSQL Instance {{ $labels.job }} is over 90% towards emergency autovacuum processes beginning'
      summary: 'PGSQL Instance emergency vacuum imminent'

  - alert: PGArchiveCommandStatus
    expr: ccp_archive_command_status_seconds_since_last_fail > 300
    for: 60s
    labels:
        service: postgresql
        severity: critical
        severity_num: 300
    annotations:
        description: 'PGSQL Instance {{ $labels.job }} has a recent failing archive command'
        summary: 'Seconds since the last recorded failure of the archive_command'

  - alert: PGSequenceExhaustion
    expr: ccp_sequence_exhaustion_count > 0
    for: 60s
    labels:
        service: postgresql
        severity: critical
        severity_num: 300
    annotations:
        description: 'Count of sequences on instance {{ $labels.job }} at over 75% usage: {{ $value }}. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent >= 75'



########## SYSTEM RULES ##########
  - alert: ExporterDown
    expr: avg_over_time(up[5m]) < 0.9 
    for: 10s 
    labels:
      service: system
      severity: critical
      severity_num: 300
    annotations:
      description: 'Metrics exporter service for {{ $labels.job }} running on {{ $labels.instance }} has been down at least 50% of the time for the last 5 minutes. Service may be flapping or down.'
      summary: 'Prometheus Exporter Service Down'

  - alert: DiskUsagePerc
    expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}) BY (job,device)) > 70
    for: 2m
    labels:
      service: system
      severity: warning
      severity_num: 200
    annotations:
      description: 'Disk usage on target {{ $labels.job }} at {{ $value }}%'

  - alert: DiskUsagePerc
    expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}) BY (job,device)) > 85
    for: 2m
    labels:
      service: system
      severity: critical
      severity_num: 300
    annotations:
      description: 'Disk usage on target {{ $labels.job }} at {{ $value }}%'

  - alert: DiskFillPredict
    expr: predict_linear(node_filesystem_free_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext"}[1h], 4 * 3600) < 0
    for: 5m
    labels:
      service: system
      severity: warning
      severity_num: 200
    annotations:
      description: '(EXPERIMENTAL) Disk {{ $labels.device }} on target {{ $labels.job }} is predicted to fill in 4 hrs based on current usage'

  - alert: SystemLoad5m
    expr: node_load5 > 5
    for: 10m
    labels:
      service: system
      severity: warning
      severity_num: 200
    annotations:
      description: 'System load for target {{ $labels.job }} is high ({{ $value }})'

  - alert: SystemLoad5m
    expr: node_load5 > 10
    for: 10m
    labels:
      service: system
      severity: critical
      severity_num: 300
    annotations:
      description: 'System load for target {{ $labels.job }} is high ({{ $value }})'

  - alert: MemoryAvailable
    expr: (100 * (node_memory_Available_bytes) / node_memory_MemTotal_bytes) < 25
    for: 1m
    labels:
      service: system
      severity: warning
      severity_num: 200
    annotations:
      description: 'Memory available for target {{ $labels.job }} is at {{ $value }}%'

  - alert: MemoryAvailable
    expr: (100 * (node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) < 10
    for: 1m
    labels:
      service: system
      severity: critical
      severity_num: 300
    annotations:
      description: 'Memory available for target {{ $labels.job }} is at {{ $value }}%'

  - alert: SwapUsage
    expr: (100 - (100 * (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes))) > 60
    for: 1m
    labels:
      service: system
      severity: warning
      severity_num: 200
    annotations:
      description: 'Swap usage for target {{ $labels.job }} is at {{ $value }}%'

  - alert: SwapUsage
    expr: (100 - (100 * (node_memory_SwapFree_byte / node_memory_SwapTotal_bytes))) > 80
    for: 1m
    labels:
      service: system
      severity: critical
      severity_num: 300
    annotations:
      description: 'Swap usage for target {{ $labels.job }} is at {{ $value }}%'


########## PGBACKREST RULES ##########
#
# Uncomment and customize one or more of these rules to monitor your pgbackrest backups. 
# Full backups are considered the equivalent of both differentials and incrementals since both are based on the last full
#   And differentials are considered incrementals since incrementals will be based off the last diff if one exists
#   This avoid false alerts, for example when you don't run diff/incr backups on the days that you run a full
# Stanza should also be set if different intervals are expected for each stanza. 
#   Otherwise rule will be applied to all stanzas returned on target system if not set.
# Otherwise, all backups returned by the pgbackrest info command run from where the database exists will be checked
#
# Relevant metric names are: 
#   ccp_backrest_last_full_time_since_completion_seconds
#   ccp_backrest_last_incr_time_since_completion_seconds
#   ccp_backrest_last_diff_time_since_completion_seconds
#
#  - alert: PGBackRestLastCompletedFull_main
#    expr: ccp_backrest_last_full_backup_time_since_completion_seconds{stanza="main"} > 604800
#    for: 60s
#    labels:
#       service: postgresql
#       severity: critical
#       severity_num: 300
#    annotations:
#       summary: 'Full backup for stanza [main] on system {{ $labels.job }} has not completed in the last week.'
#
#  - alert: PGBackRestLastCompletedIncr_main
#    expr: ccp_backrest_last_incr_backup_time_since_completion_seconds{stanza="main"} > 86400
#    for: 60s
#    labels:
#       service: postgresql
#       severity: critical
#       severity_num: 300
#    annotations:
#       summary: 'Incremental backup for stanza [main] on system {{ $labels.job }} has not completed in the last 24 hours.'
#
#
# Runtime monitoring is handled with a single metric:
#
#   ccp_backrest_last_runtime_backup_runtime_seconds
#
# Runtime monitoring should have the "backup_type" label set. 
#   Otherwise the rule will apply to the last run of all backup types returned (full, diff, incr)
# Stanza should also be set if runtimes per stanza have different expected times
#
#  - alert: PGBackRestLastRuntimeFull_main
#    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="full", stanza="main"} > 14400
#    for: 60s
#    labels:
#       service: postgresql
#       severity: critical
#       severity_num: 300
#    annotations:
#       summary: 'Expected runtime of full backup for stanza [main] has exceeded 4 hours'
#
#  - alert: PGBackRestLastRuntimeDiff_main
#    expr: ccp_backrest_last_runtime_backup_runtime_seconds{backup_type="diff", stanza="main"} > 3600
#    for: 60s
#    labels:
#       service: postgresql
#       severity: critical
#       severity_num: 300
#    annotations:
#       summary: 'Expected runtime of diff backup for stanza [main] has exceeded 1 hour'
##
#
## If the pgbackrest command fails to run, the metric disappears from the exporter output and the alert never fires. 
## An absence alert must be configured explicitly for each target (job) that backups are being monitored.
## Checking for absence of just the full backup type should be sufficient (no need for diff/incr).
## Note that while the backrest check command failing will likely also cause a scrape error alert, the addition of this 
## check gives a clearer answer as to what is causing it and that something is wrong with the backups.
#
#  - alert: PGBackrestAbsentFull_Prod
#    expr: absent(ccp_backrest_last_full_backup_time_since_completion_seconds{job="Prod"})
#    for: 10s
#    labels:
#      service: postgresql
#      severity: critical
#      severity_num: 300
#    annotations:
#      description: 'Backup Full status missing for Prod. Check that pgbackrest info command is working on target system.'


